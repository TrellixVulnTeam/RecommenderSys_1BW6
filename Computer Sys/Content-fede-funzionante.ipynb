{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import csv\n",
    "from scipy import linalg, sparse\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_rdd = sc.textFile(\"data/train.csv\")\n",
    "icm_rdd = sc.textFile(\"data/icm_fede.csv\")\n",
    "test_rdd= sc.textFile(\"data/target_users.csv\")\n",
    "\n",
    "train_header = train_rdd.first()\n",
    "icm_header = icm_rdd.first()\n",
    "test_header= test_rdd.first()\n",
    "\n",
    "train_clean_data = train_rdd.filter(lambda x: x != train_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
    "icm_clean_data = icm_rdd.filter(lambda x: x != icm_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1])))\n",
    "test_clean_data= test_rdd.filter(lambda x: x != test_header).map(lambda line: line.split(','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_users=test_clean_data.map( lambda x: int(x[0])).collect()\n",
    "#test_users=[1,2,3,4]\n",
    "#test_users.take(10)\n",
    "\n",
    "#for every item all its features\n",
    "#rouped_features = sc.parallelize([(1,[1,2]),(2,[2,3,4]),(3,[3,4]),(4,[1,2,4])])\n",
    "grouped_features = icm_clean_data.map(lambda x: (x[0],x[1])).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "#grouped_features.take(10)\n",
    "grouped_features.cache()\n",
    "total_items = grouped_features.count()\n",
    "grouped_features_arr = grouped_features.collect()\n",
    "grouped_features_dic = sc.broadcast(dict(grouped_features.collect()))\n",
    "\n",
    "tf_grouped_features = grouped_features.map(lambda x: (x[0], x[1], 1/ np.sqrt(len(x[1])))).map(lambda x: (x[0], [(item, x[2]) for item in x[1]]))\n",
    "tf_grouped_features_dic = sc.broadcast(dict(tf_grouped_features.collect()))\n",
    "\n",
    "tf_item = tf_grouped_features.map(lambda x: (x[0], x[1][0][1])).collect()\n",
    "tf_item_dic = dict(tf_item)\n",
    "\n",
    "#for every features all its items\n",
    "#grouped_items = sc.parallelize([(1,[1,4]),(2,[1,2,4]),(3,[2,3]),(4,[2,3,4])])\n",
    "grouped_items = icm_clean_data.map(lambda x: (x[1], x[0])).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "grouped_items.cache()\n",
    "grouped_items_dic = dict(grouped_items.collect())\n",
    "\n",
    "idf_features = sc.broadcast(dict(grouped_items.map(lambda x: (x[0], np.log10(total_items / len(x[1])))).collect()))\n",
    "\n",
    "def group_items_tf(f_items):\n",
    "    feature = f_items[0]\n",
    "    items = f_items[1]\n",
    "    return (feature, [(i, tf_item_dic.get(i, 0)) for i in items])\n",
    "tf_grouped_items = dict(grouped_items.map(group_items_tf).collect())\n",
    "\n",
    "#for every user all its ratings (item, rate)\n",
    "#grouped_rates = sc.parallelize([(1,[(1,8),(3,2)]),(2,[(1,2),(2,9),(3,7)]),(3,[(3,1),(4,10)])])\n",
    "grouped_rates = train_clean_data.map(lambda x: (x[0],(x[1], x[2]))).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "grouped_rates.cache()\n",
    "grouped_rates_dic = dict(train_clean_data.map(lambda x: (x[0],x[1])).groupByKey().map(lambda x: (x[0], list(x[1]))).collect())\n",
    "#for every item all its ratings\n",
    "item_ratings = train_clean_data.map(lambda x: (x[1], x[2])).aggregateByKey((0,0), lambda x,y: (x[0] + y, x[1] + 1),lambda x,y: (x[0] + y[0], x[1] + y[1]))#.sortBy(lambda x: x[1][1], ascending=False)\n",
    "#item_ratings.take(10)\n",
    "shrinkage_factor = 5\n",
    "item_ratings_mean = item_ratings.mapValues(lambda x: (x[0] / (x[1] + shrinkage_factor))).sortBy(lambda x: x[1], ascending = False).map(lambda x: x[0]).collect()\n",
    "#.map(lambda x: x[0])\n",
    "#return only test users\n",
    "def is_in_test(user):\n",
    "    return user[0] in test_users\n",
    "\n",
    "test_user_ratings = grouped_rates.filter(is_in_test).sortByKey()\n",
    "test_user_ratings.cache()\n",
    "\n",
    "test_voted_items = test_user_ratings.map(lambda x: (x[0], [item for item, rate in x[1]])).collect()\n",
    "test_voted_items_dic = dict(test_voted_items)\n",
    "\n",
    "test_user_features = grouped_rates.map(lambda x: (x[0], [grouped_features_dic.value.get(item, []) for item, rating in x[1]])).map(lambda x: (x[0], set(reduce(lambda x,y: x+y, x[1]))))\n",
    "test_user_features_dic = sc.broadcast(dict(test_user_features.collect()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#test_user_features_dic.value\n",
    "def calculate_ratings(user_rates):\n",
    "    user_id = user_rates[0]\n",
    "    i_rates = user_rates[1]\n",
    "    result = list()\n",
    "    for item, rating in i_rates:\n",
    "        result += list(map(lambda x: (x[0], rating * x[1]), tf_grouped_features_dic.value.get(item, [])))\n",
    "    result.sort(key = lambda x: x[0])\n",
    "    result = [(x,sum([z[1] for z in y])) for x,y in groupby(result, itemgetter(0))]\n",
    "    return (user_id, result)\n",
    "\n",
    "def calculate_user_idf(user_tf):\n",
    "    user_id = user_tf[0]\n",
    "    ratings = user_tf[1]\n",
    "    result = list()\n",
    "    for feature, rating in ratings:\n",
    "        result += [(feature, rating * idf_features.value.get(feature, 0))]\n",
    "    return(user_id, result)\n",
    "\n",
    "def calculate_final_percentages(user_tf, n):\n",
    "    user_id = user_tf[0]\n",
    "    print(user_id)\n",
    "    ratings = user_tf[1]\n",
    "    items_dict = defaultdict(int)\n",
    "    already_voted = grouped_rates_dic[user_id]\n",
    "    for feature, rating in ratings:\n",
    "        items_with_f = tf_grouped_items.get(feature, [])\n",
    "        for item, tf in items_with_f:\n",
    "            items_dict[item] += tf * rating\n",
    "\n",
    "    scored_items = [(total,item) for item,total in items_dict.items() if total != 0 and not item in already_voted]\n",
    "\n",
    "    # sort the scored items in ascending order\n",
    "    scored_items.sort(reverse=True)\n",
    "\n",
    "    # take out the item score\n",
    "    ranked_items = [x[1] for x in scored_items]\n",
    "    #ranked_items = scored_items\n",
    "    if n == -1:\n",
    "        return user_id,ranked_items\n",
    "    return user_id,ranked_items[:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "users_tf = test_user_ratings.map(calculate_ratings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_final_ratings = users_tf.map(calculate_user_idf).map(lambda x: calculate_final_percentages(x, 5)).collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('submission2.csv', 'wt')\n",
    "writer = csv.writer(f)\n",
    "writer.writerow(('userId','RecommendedItemIds'))\n",
    "#i = 0\n",
    "for u in users_final_ratings:\n",
    "    already_voted = grouped_rates_dic[u[0]]\n",
    "    predictions = u[1]\n",
    "    #.map(lambda x: x[0])\n",
    "    #max_index = my_list.index(max_value)\n",
    "    iterator = 0\n",
    "    for i in range(5 - len(predictions)):\n",
    "        while (item_ratings_mean[iterator] in already_voted) or (item_ratings_mean[iterator] in predictions):\n",
    "            iterator = iterator + 1\n",
    "        predictions = predictions + [item_ratings_mean[iterator]]\n",
    "    writer.writerow((u[0], '{0} {1} {2} {3} {4}'.format(predictions[0], predictions[1], predictions[2], predictions[3], predictions[4])))\n",
    "    #i+=1\n",
    "    #print(i)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
