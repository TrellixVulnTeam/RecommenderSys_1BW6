{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import csv\n",
    "from scipy import linalg, sparse\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from itertools import combinations, permutations\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "def findItemPairs(feature_id,items):\n",
    "    '''\n",
    "    For each user, find all item-item pairs combos. (i.e. items with the same user)\n",
    "    '''\n",
    "    result = list()\n",
    "    for item1,item2 in permutations(items,2):\n",
    "        result += [((item1[0],item2[0]),(item1[1],item2[1]))]\n",
    "    return result\n",
    "\n",
    "def calcSim(item_pair,rating_pairs):\n",
    "    '''\n",
    "    For each item-item pair, return the specified similarity measure,\n",
    "    along with co_raters_count\n",
    "    '''\n",
    "    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (0.0, 0.0, 0.0, 0.0, 0.0, 0)\n",
    "\n",
    "    for rating_pair in rating_pairs:\n",
    "        sum_xx += np.float(rating_pair[0]) * np.float(rating_pair[0])\n",
    "        sum_yy += np.float(rating_pair[1]) * np.float(rating_pair[1])\n",
    "        sum_xy += np.float(rating_pair[0]) * np.float(rating_pair[1])\n",
    "        # sum_y += rt[1]\n",
    "        # sum_x += rt[0]\n",
    "        n += 1\n",
    "\n",
    "    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))\n",
    "    return item_pair, (cos_sim,n)\n",
    "\n",
    "shrinkage_factor_cosine = 4\n",
    "\n",
    "def cosine(dot_product,rating_norm_squared,rating2_norm_squared):\n",
    "    '''\n",
    "    The cosine between two vectors A, B\n",
    "       dotProduct(A, B) / (norm(A) * norm(B))\n",
    "    '''\n",
    "    numerator = dot_product\n",
    "    denominator = rating_norm_squared * rating2_norm_squared + shrinkage_factor_cosine\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def correlation(size, dot_product, rating_sum, \\\n",
    "            rating2sum, rating_norm_squared, rating2_norm_squared):\n",
    "    '''\n",
    "    The correlation between two vectors A, B is\n",
    "      [n * dotProduct(A, B) - sum(A) * sum(B)] /\n",
    "        sqrt{ [n * norm(A)^2 - sum(A)^2] [n * norm(B)^2 - sum(B)^2] }\n",
    "    '''\n",
    "    numerator = size * dot_product - rating_sum * rating2sum\n",
    "    denominator = sqrt(size * rating_norm_squared - rating_sum * rating_sum) * \\\n",
    "                    sqrt(size * rating2_norm_squared - rating2sum * rating2sum)\n",
    "\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def keyOnFirstItem(item_pair,item_sim_data):\n",
    "    '''\n",
    "    For each item-item pair, make the first item's id the key\n",
    "    '''\n",
    "    (item1_id,item2_id) = item_pair\n",
    "    return item1_id,(item2_id,item_sim_data)\n",
    "\n",
    "def nearestNeighbors(item_id,items_and_sims,n):\n",
    "    '''\n",
    "    Sort the predictions list by similarity and select the top-N neighbors\n",
    "    '''\n",
    "    items_and_sims.sort(key=lambda x: x[1][0],reverse=True)\n",
    "    return item_id, items_and_sims[:n]\n",
    "\n",
    "def topNRecommendations(user_id,items_with_rating,item_sims,n):\n",
    "    '''\n",
    "    Calculate the top-N item recommendations for each user using the\n",
    "    weighted sums method\n",
    "    '''\n",
    "\n",
    "    # initialize dicts to store the score of each individual item,\n",
    "    # since an item can exist in more than one item neighborhood\n",
    "    totals = defaultdict(int)\n",
    "    sim_sums = defaultdict(int)\n",
    "    already_voted = grouped_rates_dic[user_id]\n",
    "\n",
    "    '''\n",
    "    items_ratings = dict(items_with_rating)\n",
    "    for item in item_sims.keys():\n",
    "        nearest_neighbors = item_sims.get(item,None)\n",
    "        if nearest_neighbors:\n",
    "            for (neighbor,(sim,count)) in nearest_neighbors:\n",
    "                rating = items_ratings.get(item, 0)\n",
    "                if rating != 0:\n",
    "                    totals[neighbor] += sim * rating\n",
    "                    sim_sums[neighbor] += sim\n",
    "                rating_neighbor = items_ratings.get(neighbor, 0)\n",
    "                if rating_neighbor != 0:\n",
    "                    totals[item] += sim * rating_neighbor\n",
    "                    sim_sums[item] += sim\n",
    "    '''\n",
    "    for (item,rating) in items_with_rating:\n",
    "\n",
    "        # lookup the nearest neighbors for this item\n",
    "        nearest_neighbors = item_sims.get(item,None)\n",
    "        if nearest_neighbors:\n",
    "            for (neighbor,(sim,count)) in nearest_neighbors:\n",
    "                if neighbor != item:\n",
    "\n",
    "                    # update totals and sim_sums with the rating data\n",
    "                    totals[neighbor] += sim * rating\n",
    "                    sim_sums[neighbor] += sim\n",
    "\n",
    "    # create the normalized list of scored items\n",
    "    #/sim_sums[item]\n",
    "    scored_items = [(total,item) for item,total in totals.items() if sim_sums[item] != 0 and not item in already_voted]\n",
    "\n",
    "    # sort the scored items in ascending order\n",
    "    scored_items.sort(reverse=True)\n",
    "\n",
    "    # take out the item score\n",
    "    #ranked_items = [x[1] for x in scored_items]\n",
    "    ranked_items = scored_items\n",
    "    if n == -1:\n",
    "        return user_id,ranked_items\n",
    "    return user_id,ranked_items[:n]\n",
    "\n",
    "\n",
    "train_rdd = sc.textFile(\"data/train.csv\")\n",
    "icm_rdd = sc.textFile(\"data/icm_fede.csv\")\n",
    "test_rdd= sc.textFile(\"data/target_users.csv\")\n",
    "\n",
    "train_header = train_rdd.first()\n",
    "icm_header = icm_rdd.first()\n",
    "test_header= test_rdd.first()\n",
    "\n",
    "train_clean_data = train_rdd.filter(lambda x: x != train_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
    "icm_clean_data = icm_rdd.filter(lambda x: x != icm_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1])))\n",
    "test_clean_data= test_rdd.filter(lambda x: x != test_header).map(lambda line: line.split(','))\n",
    "\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "test_users=test_clean_data.map( lambda x: int(x[0])).collect()\n",
    "#test_users=[1,2,3,4]\n",
    "#test_users.take(10)\n",
    "\n",
    "#for every item all its features\n",
    "#rouped_features = sc.parallelize([(1,[1,2]),(2,[2,3,4]),(3,[3,4]),(4,[1,2,4])])\n",
    "grouped_features = icm_clean_data.map(lambda x: (x[0],x[1])).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "#grouped_features.take(10)\n",
    "grouped_features.cache()\n",
    "total_items = grouped_features.count()\n",
    "grouped_features_arr = grouped_features.collect()\n",
    "grouped_features_dic = sc.broadcast(dict(grouped_features.collect()))\n",
    "\n",
    "tf_grouped_features = grouped_features.map(lambda x: (x[0], x[1], 1/ np.sqrt(len(x[1])))).map(lambda x: (x[0], [(item, x[2]) for item in x[1]]))\n",
    "tf_grouped_features_dic = sc.broadcast(dict(tf_grouped_features.collect()))\n",
    "\n",
    "tf_item = tf_grouped_features.map(lambda x: (x[0], x[1][0][1])).collect()\n",
    "tf_item_dic = dict(tf_item)\n",
    "\n",
    "#for every features all its items\n",
    "#grouped_items = sc.parallelize([(1,[1,4]),(2,[1,2,4]),(3,[2,3]),(4,[2,3,4])])\n",
    "grouped_items = icm_clean_data.map(lambda x: (x[1], x[0])).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "grouped_items.cache()\n",
    "grouped_items_dic = dict(grouped_items.collect())\n",
    "idf_features = sc.broadcast(dict(grouped_items.map(lambda x: (x[0], np.log10(total_items / len(x[1])))).collect()))\n",
    "idf_features.value.get(1)\n",
    "def tf_idf(item_features):\n",
    "    item_id = item_features[0]\n",
    "    result = list()\n",
    "    for feature, tf in item_features[1]:\n",
    "        result += [(feature, tf * idf_features.value.get(feature))]\n",
    "    return item_id, result\n",
    "\n",
    "tf_idf_items = tf_grouped_features.map(tf_idf).cache()\n",
    "\n",
    "def group_items_tf(f_items):\n",
    "    feature = f_items[0]\n",
    "    items = f_items[1]\n",
    "    return (feature, [(i, tf_item_dic.get(i, 0)) for i in items])\n",
    "tf_grouped_items = dict(grouped_items.map(group_items_tf).collect())\n",
    "\n",
    "#for every user all its ratings (item, rate)\n",
    "#grouped_rates = sc.parallelize([(1,[(1,8),(3,2)]),(2,[(1,2),(2,9),(3,7)]),(3,[(3,1),(4,10)])])\n",
    "grouped_rates = train_clean_data.map(lambda x: (x[0],(x[1], x[2]))).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "grouped_rates.cache()\n",
    "grouped_rates_dic = dict(train_clean_data.map(lambda x: (x[0],x[1])).groupByKey().map(lambda x: (x[0], list(x[1]))).collect())\n",
    "#for every item all its ratings\n",
    "item_ratings = train_clean_data.map(lambda x: (x[1], x[2])).aggregateByKey((0,0), lambda x,y: (x[0] + y, x[1] + 1),lambda x,y: (x[0] + y[0], x[1] + y[1]))#.sortBy(lambda x: x[1][1], ascending=False)\n",
    "#item_ratings.take(10)\n",
    "shrinkage_factor = 5\n",
    "item_ratings_mean = item_ratings.mapValues(lambda x: (x[0] / (x[1] + shrinkage_factor))).sortBy(lambda x: x[1], ascending = False).map(lambda x: x[0]).collect()\n",
    "#.map(lambda x: x[0])\n",
    "#return only test users\n",
    "def is_in_test(user):\n",
    "    return user[0] in test_users\n",
    "\n",
    "test_user_ratings = grouped_rates.filter(is_in_test).sortByKey()\n",
    "test_user_ratings.cache()\n",
    "\n",
    "test_voted_items = test_user_ratings.map(lambda x: (x[0], [item for item, rate in x[1]])).collect()\n",
    "test_voted_items_dic = dict(test_voted_items)\n",
    "\n",
    "test_user_features = grouped_rates.map(lambda x: (x[0], [grouped_features_dic.value.get(item, []) for item, rating in x[1]])).map(lambda x: (x[0], set(reduce(lambda x,y: x+y, x[1]))))\n",
    "test_user_features_dic = sc.broadcast(dict(test_user_features.collect()))\n",
    "\n",
    "grouped_items_f = tf_idf_items.flatMap(lambda x: [(f, (x[0], tf_idf_f)) for f, tf_idf_f in x[1]]).groupByKey().map(lambda x: (x[0], list(x[1]))).cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairwise_items = grouped_items_f.flatMap(\n",
    "    lambda p: findItemPairs(p[0],p[1])).groupByKey()\n",
    "#Calculate the cosine similarity for each item pair and select the top-N nearest neighbors:(item1,item2) ->    (similarity,co_raters_count)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-64f806205999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m item_sims = pairwise_items.map(\n\u001b[0;32m----> 2\u001b[0;31m     lambda p: calcSim(p[0],list(p[1]))).map(lambda p: keyOnFirstItem(p[0],p[1])).groupByKey().map(lambda p: nearestNeighbors(p[0],list(p[1]),50000)).collect()\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Preprocess the item similarity matrix into a dictionary and store it as a broadcast variable:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#item_sims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \"\"\"\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "item_sims = pairwise_items.map(\n",
    "    lambda p: calcSim(p[0],list(p[1]))).map(lambda p: keyOnFirstItem(p[0],p[1])).groupByKey().map(lambda p: nearestNeighbors(p[0],list(p[1]),50000)).collect()\n",
    "#Preprocess the item similarity matrix into a dictionary and store it as a broadcast variable:\n",
    "#item_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_sim_dict = {}\n",
    "for (item,data) in item_sims:\n",
    "    item_sim_dict[item] = data\n",
    "\n",
    "isb = sc.broadcast(item_sim_dict)\n",
    "#Calculate the top-N item recommendations for each user user_id -> [item1,item2,item3,...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user_item_recs = user_item_pairs.filter(lambda x: x[0] in test_users).map(lambda p: topNRecommendations(p[0],p[1],isb.value,5)).sortByKey().collect()\n",
    "user_item_recs = train_clean_data.filter(lambda x: x[0] in test_users).map(lambda x: (x[0], (x[1], x[2]))).groupByKey().map(lambda p: (p[0],list(p[1]))).map(lambda p: topNRecommendations(p[0],p[1],isb.value,5)).sortByKey().collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../submission2.csv', 'wt')\n",
    "\n",
    "writer = csv.writer(f)\n",
    "writer.writerow(('userId','RecommendedItemIds'))\n",
    "\n",
    "for u in user_item_recs:\n",
    "    predictions = u[1]\n",
    "    iterator = 0\n",
    "    already_voted = grouped_rates_dic[u[0]]\n",
    "    for i in range(5 - len(predictions)):\n",
    "        while (item_ratings_mean[iterator] in already_voted) or (item_ratings_mean[iterator] in predictions):\n",
    "            iterator = iterator + 1\n",
    "        predictions = predictions + [item_ratings_mean[iterator]]\n",
    "    writer.writerow((u[0], '{0} {1} {2} {3} {4}'.format(predictions[0], predictions[1], predictions[2], predictions[3], predictions[4])))\n",
    "    #i+=1\n",
    "    #print(i)\n",
    "\n",
    "f.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
