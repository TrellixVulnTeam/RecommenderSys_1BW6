{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import csv\n",
    "from scipy import linalg, sparse\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from itertools import combinations, permutations\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "def findItemPairs(feature_id,items):\n",
    "    '''\n",
    "    For each user, find all item-item pairs combos. (i.e. items with the same user)\n",
    "    '''\n",
    "    result = list()\n",
    "    for item1,item2 in permutations(items,2):\n",
    "        result += [((item1[0],item2[0]),(item1[1],item2[1]))]\n",
    "    return result\n",
    "\n",
    "def calcSim(item_pair,rating_pairs):\n",
    "    '''\n",
    "    For each item-item pair, return the specified similarity measure,\n",
    "    along with co_raters_count\n",
    "    '''\n",
    "    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (0.0, 0.0, 0.0, 0.0, 0.0, 0)\n",
    "\n",
    "    for rating_pair in rating_pairs:\n",
    "        sum_xx += np.float(rating_pair[0]) * np.float(rating_pair[0])\n",
    "        sum_yy += np.float(rating_pair[1]) * np.float(rating_pair[1])\n",
    "        sum_xy += np.float(rating_pair[0]) * np.float(rating_pair[1])\n",
    "        # sum_y += rt[1]\n",
    "        # sum_x += rt[0]\n",
    "        n += 1\n",
    "\n",
    "    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))\n",
    "    return item_pair, (cos_sim,n)\n",
    "\n",
    "shrinkage_factor_cosine = 4\n",
    "\n",
    "def cosine(dot_product,rating_norm_squared,rating2_norm_squared):\n",
    "    '''\n",
    "    The cosine between two vectors A, B\n",
    "       dotProduct(A, B) / (norm(A) * norm(B))\n",
    "    '''\n",
    "    numerator = dot_product\n",
    "    denominator = rating_norm_squared * rating2_norm_squared + shrinkage_factor_cosine\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def correlation(size, dot_product, rating_sum, \\\n",
    "            rating2sum, rating_norm_squared, rating2_norm_squared):\n",
    "    '''\n",
    "    The correlation between two vectors A, B is\n",
    "      [n * dotProduct(A, B) - sum(A) * sum(B)] /\n",
    "        sqrt{ [n * norm(A)^2 - sum(A)^2] [n * norm(B)^2 - sum(B)^2] }\n",
    "    '''\n",
    "    numerator = size * dot_product - rating_sum * rating2sum\n",
    "    denominator = sqrt(size * rating_norm_squared - rating_sum * rating_sum) * \\\n",
    "                    sqrt(size * rating2_norm_squared - rating2sum * rating2sum)\n",
    "\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def keyOnFirstItem(item_pair,item_sim_data):\n",
    "    '''\n",
    "    For each item-item pair, make the first item's id the key\n",
    "    '''\n",
    "    (item1_id,item2_id) = item_pair\n",
    "    return item1_id,(item2_id,item_sim_data)\n",
    "\n",
    "def nearestNeighbors(item_id,items_and_sims,n):\n",
    "    '''\n",
    "    Sort the predictions list by similarity and select the top-N neighbors\n",
    "    '''\n",
    "    items_and_sims.sort(key=lambda x: x[1][0],reverse=True)\n",
    "    return item_id, items_and_sims[:n]\n",
    "\n",
    "def topNRecommendations(user_id,items_with_rating,item_sims,n):\n",
    "    '''\n",
    "    Calculate the top-N item recommendations for each user using the\n",
    "    weighted sums method\n",
    "    '''\n",
    "\n",
    "    # initialize dicts to store the score of each individual item,\n",
    "    # since an item can exist in more than one item neighborhood\n",
    "    totals = defaultdict(int)\n",
    "    sim_sums = defaultdict(int)\n",
    "    already_voted = grouped_rates_dic[user_id]\n",
    "\n",
    "    '''\n",
    "    items_ratings = dict(items_with_rating)\n",
    "    for item in item_sims.keys():\n",
    "        nearest_neighbors = item_sims.get(item,None)\n",
    "        if nearest_neighbors:\n",
    "            for (neighbor,(sim,count)) in nearest_neighbors:\n",
    "                rating = items_ratings.get(item, 0)\n",
    "                if rating != 0:\n",
    "                    totals[neighbor] += sim * rating\n",
    "                    sim_sums[neighbor] += sim\n",
    "                rating_neighbor = items_ratings.get(neighbor, 0)\n",
    "                if rating_neighbor != 0:\n",
    "                    totals[item] += sim * rating_neighbor\n",
    "                    sim_sums[item] += sim\n",
    "    '''\n",
    "    for (item,rating) in items_with_rating:\n",
    "\n",
    "        # lookup the nearest neighbors for this item\n",
    "        nearest_neighbors = item_sims.get(item,None)\n",
    "        if nearest_neighbors:\n",
    "            for (neighbor,(sim,count)) in nearest_neighbors:\n",
    "                if neighbor != item:\n",
    "\n",
    "                    # update totals and sim_sums with the rating data\n",
    "                    totals[neighbor] += sim * rating\n",
    "                    sim_sums[neighbor] += sim\n",
    "\n",
    "    # create the normalized list of scored items\n",
    "    #/sim_sums[item]\n",
    "    scored_items = [(total,item) for item,total in totals.items() if sim_sums[item] != 0 and not item in already_voted]\n",
    "\n",
    "    # sort the scored items in ascending order\n",
    "    scored_items.sort(reverse=True)\n",
    "\n",
    "    # take out the item score\n",
    "    #ranked_items = [x[1] for x in scored_items]\n",
    "    ranked_items = scored_items\n",
    "    if n == -1:\n",
    "        return user_id,ranked_items\n",
    "    return user_id,ranked_items[:n]\n",
    "\n",
    "\n",
    "train_rdd = sc.textFile(\"data/train.csv\")\n",
    "icm_rdd = sc.textFile(\"data/icm_fede.csv\")\n",
    "test_rdd= sc.textFile(\"data/target_users.csv\")\n",
    "\n",
    "train_header = train_rdd.first()\n",
    "icm_header = icm_rdd.first()\n",
    "test_header= test_rdd.first()\n",
    "\n",
    "train_clean_data = train_rdd.filter(lambda x: x != train_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
    "icm_clean_data = icm_rdd.filter(lambda x: x != icm_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1])))\n",
    "test_clean_data= test_rdd.filter(lambda x: x != test_header).map(lambda line: line.split(','))\n",
    "\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "test_users=test_clean_data.map( lambda x: int(x[0])).collect()\n",
    "#test_users=[1,2,3,4]\n",
    "#test_users.take(10)\n",
    "\n",
    "#for every item all its features\n",
    "#rouped_features = sc.parallelize([(1,[1,2]),(2,[2,3,4]),(3,[3,4]),(4,[1,2,4])])\n",
    "grouped_features = icm_clean_data.map(lambda x: (x[0],x[1])).groupByKey().map(lambda x: (x[0], list(x[1]))).cache()\n",
    "\n",
    "total_items = grouped_features.count()\n",
    "\n",
    "tf_grouped_features = grouped_features.map(lambda x: (x[0], x[1], 1/ np.sqrt(len(x[1])))).map(lambda x: (x[0], [(item, x[2]) for item in x[1]])).cache()\n",
    "\n",
    "tf_item = tf_grouped_features.map(lambda x: (x[0], x[1][0][1])).collect()\n",
    "tf_item_dic = dict(tf_item)\n",
    "\n",
    "#for every features all its items\n",
    "#grouped_items = sc.parallelize([(1,[1,4]),(2,[1,2,4]),(3,[2,3]),(4,[2,3,4])])\n",
    "grouped_items = icm_clean_data.map(lambda x: (x[1], x[0])).groupByKey().map(lambda x: (x[0], list(x[1]))).cache()\n",
    "\n",
    "idf_features = sc.broadcast(dict(grouped_items.map(lambda x: (x[0], np.log10(total_items / len(x[1])))).collect()))\n",
    "\n",
    "def tf_idf(item_features):\n",
    "    item_id = item_features[0]\n",
    "    result = list()\n",
    "    for feature, tf in item_features[1]:\n",
    "        result += [(feature, tf * idf_features.value.get(feature))]\n",
    "    return item_id, result\n",
    "\n",
    "tf_idf_items = tf_grouped_features.map(tf_idf).cache()\n",
    "\n",
    "def group_items_tf(f_items):\n",
    "    feature = f_items[0]\n",
    "    items = f_items[1]\n",
    "    return (feature, [(i, tf_item_dic.get(i, 0)) for i in items])\n",
    "tf_grouped_items = dict(grouped_items.map(group_items_tf).collect())\n",
    "\n",
    "#for every user all its ratings (item, rate)\n",
    "#grouped_rates = sc.parallelize([(1,[(1,8),(3,2)]),(2,[(1,2),(2,9),(3,7)]),(3,[(3,1),(4,10)])])\n",
    "grouped_rates_dic = dict(train_clean_data.map(lambda x: (x[0],x[1])).groupByKey().map(lambda x: (x[0], list(x[1]))).collect())\n",
    "#for every item all its ratings\n",
    "item_ratings = train_clean_data.map(lambda x: (x[1], x[2])).aggregateByKey((0,0), lambda x,y: (x[0] + y, x[1] + 1),lambda x,y: (x[0] + y[0], x[1] + y[1]))#.sortBy(lambda x: x[1][1], ascending=False)\n",
    "#item_ratings.take(10)\n",
    "shrinkage_factor = 5\n",
    "item_ratings_mean = item_ratings.mapValues(lambda x: (x[0] / (x[1] + shrinkage_factor))).sortBy(lambda x: x[1], ascending = False).map(lambda x: x[0]).collect()\n",
    "#.map(lambda x: x[0])\n",
    "#return only test users\n",
    "\n",
    "grouped_items_f = tf_idf_items.flatMap(lambda x: [(f, (x[0], tf_idf_f)) for f, tf_idf_f in x[1]]).groupByKey().map(lambda x: (x[0], list(x[1]))).cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairwise_items = grouped_items_f.flatMap(\n",
    "    lambda p: findItemPairs(p[0],p[1])).groupByKey().cache()\n",
    "#Calculate the cosine similarity for each item pair and select the top-N nearest neighbors:(item1,item2) ->    (similarity,co_raters_count)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_sims = pairwise_items.map(\n",
    "    lambda p: calcSim(p[0],list(p[1]))).map(lambda p: keyOnFirstItem(p[0],p[1])).groupByKey().collect().map(lambda p: nearestNeighbors(p[0],list(p[1]),50)).collect()\n",
    "#Preprocess the item similarity matrix into a dictionary and store it as a broadcast variable:\n",
    "#item_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_sim_dict = {}\n",
    "for (item,data) in item_sims:\n",
    "    item_sim_dict[item] = data\n",
    "\n",
    "isb = sc.broadcast(item_sim_dict)\n",
    "#Calculate the top-N item recommendations for each user user_id -> [item1,item2,item3,...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user_item_recs = user_item_pairs.filter(lambda x: x[0] in test_users).map(lambda p: topNRecommendations(p[0],p[1],isb.value,5)).sortByKey().collect()\n",
    "user_item_recs = train_clean_data.filter(lambda x: x[0] in test_users).map(lambda x: (x[0], (x[1], x[2]))).groupByKey().map(lambda p: (p[0],list(p[1]))).map(lambda p: topNRecommendations(p[0],p[1],isb.value,5)).sortByKey().collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('submission2.csv', 'wt')\n",
    "\n",
    "writer = csv.writer(f)\n",
    "writer.writerow(('userId','RecommendedItemIds'))\n",
    "\n",
    "for u in user_item_recs:\n",
    "    predictions = u[1]\n",
    "    iterator = 0\n",
    "    already_voted = grouped_rates_dic[u[0]]\n",
    "    for i in range(5 - len(predictions)):\n",
    "        while (item_ratings_mean[iterator] in already_voted) or (item_ratings_mean[iterator] in predictions):\n",
    "            iterator = iterator + 1\n",
    "        predictions = predictions + [item_ratings_mean[iterator]]\n",
    "    writer.writerow((u[0], '{0} {1} {2} {3} {4}'.format(predictions[0], predictions[1], predictions[2], predictions[3], predictions[4])))\n",
    "    #i+=1\n",
    "    #print(i)\n",
    "\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
