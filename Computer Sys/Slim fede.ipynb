{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from scipy import sparse as sm\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rdd = sc.textFile(\"data/train.csv\")\n",
    "icm_rdd = sc.textFile(\"data/icm_fede.csv\")\n",
    "test_rdd= sc.textFile(\"data/target_users.csv\")\n",
    "\n",
    "train_header = train_rdd.first()\n",
    "icm_header = icm_rdd.first()\n",
    "test_header= test_rdd.first()\n",
    "\n",
    "train_clean_data = train_rdd.filter(lambda x: x != train_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
    "icm_clean_data = icm_rdd.filter(lambda x: x != icm_header).map(lambda line: line.split(',')).map(lambda x: (int(x[0]), int(x[1])))\n",
    "test_clean_data= test_rdd.filter(lambda x: x != test_header).map(lambda line: line.split(','))\n",
    "\n",
    "test_users=test_clean_data.map( lambda x: int(x[0])).collect()\n",
    "\n",
    "\n",
    "grouped_rates = train_clean_data.filter(lambda x: x[0] in test_users).map(lambda x: (x[0],x[1])).groupByKey().map(lambda x: (x[0], list(x[1]))).collect()\n",
    "grouped_rates_dic = dict(grouped_rates)\n",
    "\n",
    "\n",
    "item_ratings = train_clean_data.map(lambda x: (x[0], x[2])).aggregateByKey((0,0), lambda x,y: (x[0] + y, x[1] + 1),lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "user_ratings_mean = item_ratings.mapValues(lambda x: (x[0] / (x[1]))).collect()\n",
    "user_ratings_mean_dic=dict(user_ratings_mean)\n",
    "\n",
    "\n",
    "item_ratings_forTop = train_clean_data.map(lambda x: (x[1], x[2])).aggregateByKey((0,0), lambda x,y: (x[0] + y, x[1] + 1),lambda x,y: (x[0] + y[0], x[1] + y[1]))#.sortBy(lambda x: x[1][1], ascending=False)\n",
    "#item_ratings.take(10)\n",
    "shrinkage_factor = 5\n",
    "item_ratings_mean = item_ratings_forTop.mapValues(lambda x: (x[0] / (x[1] + shrinkage_factor))).sortBy(lambda x: x[1], ascending = False).map(lambda x: x[0]).collect()\n",
    "\n",
    "def evaluateRating(tuple):\n",
    "    if tuple[2] >= 8:\n",
    "        return 1\n",
    "    return -1\n",
    "    \n",
    "users = train_clean_data.map(lambda x: x[0]).collect()\n",
    "items = train_clean_data.map(lambda x: x[1]).collect()\n",
    "ratings = train_clean_data.map(evaluateRating).collect()\n",
    "ratings_unbiased = train_clean_data.map(lambda x: x[2]-user_ratings_mean_dic[x[0]]).collect()\n",
    "\n",
    "shape = (train_clean_data.map(lambda x: int(x[0])).max()+1,\n",
    "         train_clean_data.map(lambda x: int(x[1])).max()+1)\n",
    "\n",
    "UxI = sm.csc_matrix((ratings, (users, items)), shape=shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "unos=[1]*len(items_for_features)\n",
    "\n",
    "UxI= sm.csr_matrix((ratings, (users, items)))\n",
    "UxI_coo = UxI.tocoo()\n",
    "IxF= sm.csr_matrix((unos, (items_for_features, features)))\n",
    "IxF_coo = IxF.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitOneColumn(colID, URM, l1_penalty=0.1, l2_penalty=0.1, positive_only=True):\n",
    "    \n",
    "    l1_ratio = l1_penalty / (l1_penalty + l2_penalty)\n",
    "\n",
    "    model = ElasticNet(alpha=1.0,\n",
    "                       l1_ratio=l1_ratio,\n",
    "                       positive=positive_only,\n",
    "                       fit_intercept=False,\n",
    "                       copy_X=False)\n",
    "\n",
    "    # get the target column\n",
    "    y = URM[:, colID].toarray()\n",
    "\n",
    "    # set the colID column of URM to zero\n",
    "    URM.data[URM.indptr[colID]:URM.indptr[colID + 1]] = 0.0\n",
    "\n",
    "    # fit one ElasticNet model per column\n",
    "    model.fit(URM, y)\n",
    "\n",
    "    # self.model.coef_ contains the coefficient of the ElasticNet model\n",
    "    # let's keep only the non-zero values\n",
    "    nnz_idx = model.coef_ > 0.0\n",
    "\n",
    "    values = model.coef_[nnz_idx]\n",
    "    rows = np.arange(URM.shape[1])[nnz_idx]\n",
    "    cols = np.ones(nnz_idx.sum()) * colID\n",
    "\n",
    "    \n",
    "    return list(zip(rows, cols, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitSLIM(URM):\n",
    "    \n",
    "    itemNumber = URM.shape[1]\n",
    "    \n",
    "    itemList = sc.parallelize(list(range(itemNumber)))\n",
    "    \n",
    "    # fit item's factors in parallel\n",
    "    slimResult = itemList.flatMap(lambda x: fitOneColumn(x, URM))\n",
    "\n",
    "    rows = slimResult.map(lambda x: x[0]).collect()\n",
    "    cols = slimResult.map(lambda x: x[1]).collect()\n",
    "    values = slimResult.map(lambda x: x[2]).collect()   \n",
    "    \n",
    "    # generate the sparse weight matrix\n",
    "    return sps.csc_matrix((values, (rows, cols)), \n",
    "                              shape=(itemList.max()+1, itemList.max()+1),\n",
    "                              dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similaritySLIM = fitSLIM(UxI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_recommendation(model, data, i_f, user_ids):\n",
    "\n",
    "\n",
    "    n_users, _ = data.shape\n",
    "    n_items, _ = i_f.shape\n",
    "    \n",
    "    f = open('submission_light_fede_150_30.csv', 'wt')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('userId','RecommendedItemIds'))\n",
    "\n",
    "    for user_id in tqdm(user_ids):\n",
    "        known_positives = set(data[user_id].indices)\n",
    "        scores = model.predict(user_id, np.arange(n_items), item_features=i_f)\n",
    "        top_items = np.argsort(-scores)\n",
    "        top_items = [item for item in top_items if item not in known_positives]\n",
    "       #mask = np.in1d(top_items, known_positives,invert=True)\n",
    "       #top_items = top_items[mask][:5]\n",
    "       #top=[0,0,0,0,0]\n",
    "        top=top_items[:5]\n",
    "        iterator = 0\n",
    "        for i in range(5 - len(top)):\n",
    "            prediction = item_ratings_mean[iterator]\n",
    "            while prediction in grouped_rates_dic[user] or prediction in top:\n",
    "                iterator += 1\n",
    "                prediction = item_ratings_mean[iterator]\n",
    "                iterator += 1\n",
    "            top.append(prediction)  \n",
    "        writer.writerow((user_id, '{0} {1} {2} {3} {4}'.format(top[0], top[1], top[2], top[3], top[4])))\n",
    "\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
