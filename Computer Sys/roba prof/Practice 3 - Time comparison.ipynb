{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Systems 2016/17\n",
    "\n",
    "### Practice 3 - Time Comparison\n",
    "\n",
    "#### Given a big array of numbers, say 30 million cells, we want to zero out all values below 2.5\n",
    "\n",
    "###### Please excuse the occasional humor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.6 ,  1.12,  4.96,  2.9 ,  4.31,  2.34,  4.18,  2.68,  1.39,\n",
       "        2.68,  3.47,  1.15,  1.29,  4.49,  2.02,  3.19,  3.57,  2.26,\n",
       "        1.37,  1.27,  3.54,  1.78,  2.32,  1.6 ,  3.05,  4.52,  1.89])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "numValues = 30000000\n",
    "threshold = 2.5\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fakeDataVector = np.round(np.random.rand(numValues)*4 +1, decimals=2)\n",
    "\n",
    "requiredTime_dataCreation = time.time()-start_time\n",
    "\n",
    "resultVector = np.zeros(numValues)\n",
    "\n",
    "fakeDataVector[0:27]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential processing takes 5.364 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "resultVector = fakeDataVector\n",
    "\n",
    "for i in range(len(fakeDataVector)):\n",
    "    if(fakeDataVector[i]<=threshold):\n",
    "        resultVector[i] = 0\n",
    "            \n",
    "            \n",
    "requiredTime_sequential = time.time()-start_time\n",
    "            \n",
    "print(\"Sequential processing takes {:.3f} seconds\".format(requiredTime_sequential))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized processing takes 0.144 seconds\n",
      "\n",
      "\n",
      "So, the vectorized version takes only 2.69 % of the time needed by the sequential one\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "resultVector = fakeDataVector\n",
    "\n",
    "resultVector[resultVector<=threshold] = 0\n",
    "\n",
    "requiredTime_vectorized = time.time()-start_time\n",
    "\n",
    "print(\"Vectorized processing takes {:.3f} seconds\".format(requiredTime_vectorized))\n",
    "\n",
    "print(\"\\n\\nSo, the vectorized version takes only {:.2f} % of the time needed by the sequential one\".\n",
    "      format(requiredTime_vectorized/requiredTime_sequential*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark processing takes 1.010 seconds\n",
      "\n",
      "\n",
      "No wait, what is this? \n",
      "Spark is taking 699.79 % of the time needed by the vectorized code?\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fakeDataVector_rdd = sc.parallelize(fakeDataVector)\n",
    "\n",
    "start_time_map = time.time()\n",
    "\n",
    "resultVector_rdd = fakeDataVector_rdd.map(lambda x: 0 if x<=threshold else x )\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "requiredTime_pyspark_overall = end_time-start_time\n",
    "requiredTime_pyspark_rdd = start_time_map-start_time\n",
    "requiredTime_pyspark_map = end_time-start_time_map\n",
    "\n",
    "print(\"PySpark processing takes {:.3f} seconds\".format(requiredTime_pyspark_overall))\n",
    "\n",
    "if (requiredTime_pyspark_overall>requiredTime_vectorized):\n",
    "    print(\"\\n\\nNo wait, what is this? \\nSpark is taking {:.2f} % of the time needed by the vectorized code?\".\n",
    "      format(requiredTime_pyspark_overall/requiredTime_vectorized*100))\n",
    "else:\n",
    "    print(\"Damn that was unlucky... run the previous cells again please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Plot twist... Surprised?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is, of course, a valid reason for this result... \n",
    "\n",
    "#### When using PySpark we are performing two operations\n",
    "* Transforming the data into a RDD\n",
    "* Mapping the elements into the desired values\n",
    "\n",
    "#### Let's see how the computation time is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAEyCAYAAABULszLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMJggAwAYTJABAAwmyAAABvt/c+h2mmkncV8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2594013d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "names = ['RDD conversion','MAP']\n",
    "\n",
    "data = [requiredTime_pyspark_rdd,requiredTime_pyspark_map]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.subplot(111)\n",
    "width=0.5\n",
    "\n",
    "bins = map(lambda x: x-width/2,range(1,len(data)+1))\n",
    "ax.bar(bins,data,width=width)\n",
    "ax.set_xticks(map(lambda x: x, range(1,len(data)+1)))\n",
    "ax.set_xticklabels(names)\n",
    "\n",
    "plt.ylabel(\"Seconds\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is no bar for map! Strange, perhaps we should look at the values, increasing the number of decimals..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data transformation took \t1.010128 seconds\n",
      "The map operation took \t\t0.000107 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"The data transformation took \\t{:.6f} seconds\".format(requiredTime_pyspark_rdd))\n",
    "print(\"The map operation took \\t\\t{:.6f} seconds\".format(requiredTime_pyspark_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are witnessing a data conversion/loading delay\n",
    "#### Always manage your data carefully, you don't want to waste most of the execution time transferring it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy random generator takes 0.507 seconds\n",
      "\n",
      "Generating the random numbers dyrectly into the RDD takes 0.065 seconds\n",
      "\n",
      "\n",
      "Overall PySpark processing takes 0.064750 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fakeDataVector_rdd = RandomRDDs.uniformRDD(sc, numValues).map(lambda x: x*4+1)\n",
    "\n",
    "start_time_map = time.time()\n",
    "\n",
    "resultVector_rdd = fakeDataVector_rdd.map(lambda x: 0 if x<=threshold else x )\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "requiredTime_pyspark_overall = end_time-start_time\n",
    "requiredTime_pyspark_rdd = start_time_map-start_time\n",
    "requiredTime_pyspark_map = end_time-start_time_map\n",
    "\n",
    "print(\"Numpy random generator takes {:.3f} seconds\".format(requiredTime_dataCreation))\n",
    "\n",
    "print(\"\\nGenerating the random numbers dyrectly into the RDD takes {:.3f} seconds\".format(requiredTime_pyspark_rdd))\n",
    "\n",
    "print(\"\\n\\nOverall PySpark processing takes {:.6f} seconds\".format(requiredTime_pyspark_overall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To sum up, if we focus only on the computation, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential \t5.363795 seconds\n",
      "\n",
      "\n",
      "Vectorized \t0.144362 seconds \n",
      "\t\tImprovement of 37.16 X over sequential\n",
      "\n",
      "\n",
      "PySpark \t0.000172 seconds \n",
      "\t\tImprovement of 839.80 X over vectorized \n",
      "\t\tImprovement of 31,203.03 X over sequential\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequential \\t{:.6f} seconds\".format(requiredTime_sequential))\n",
    "\n",
    "print(\"\\n\\nVectorized \\t{:.6f} seconds \\n\\t\\tImprovement of {:.2f} X over sequential\".\n",
    "      format(requiredTime_vectorized, \n",
    "             requiredTime_sequential/requiredTime_vectorized))\n",
    "\n",
    "print(\"\\n\\nPySpark \\t{:.6f} seconds \\n\\t\\tImprovement of {:,.2f} X over vectorized \\n\\t\\tImprovement of {:,.2f} X over sequential\".\n",
    "      format(requiredTime_pyspark_map, \n",
    "             requiredTime_vectorized/requiredTime_pyspark_map,\n",
    "             requiredTime_sequential/requiredTime_pyspark_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The performance gain of PySpark is HUGE... Any thougts on this result?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "#### There is an important catch. All Spark Transformations are lazy evaluated, nothing is done until the result is needed by a Spark Action. Which means that the time we got is a meaningless number. It represents only the SparkContext \"acknowleging\" that at some point in the future that operation will have to be done.\n",
    "\n",
    "#### Some Spark Transformations are:\n",
    "* map(func)\n",
    "* filter(func)\n",
    "* union(otherDataset)\n",
    "* intersection(otherDataset)\n",
    "* groupByKey([numTasks])\n",
    "* reduceByKey(func, [numTasks])\n",
    "\n",
    "#### Some Spark Action are:\n",
    "* reduce(func)\n",
    "* collect()\n",
    "* count()\n",
    "* take(n)\n",
    "* saveAsTextFile(path)\n",
    "* foreach(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can force the execution by performing a Take() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Vectorized \t0.144362 seconds \n",
      "\t\tImprovement of 37.16 X over sequential\n",
      "\n",
      "\n",
      "PySpark \t0.473484 seconds \n",
      "\t\tImprovement of 0.30 X over vectorized \n",
      "\t\tImprovement of 11.33 X over sequential\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "\n",
    "fakeDataVector_rdd = RandomRDDs.uniformRDD(sc, numValues).map(lambda x: x*4+1)\n",
    "\n",
    "start_time_map = time.time()\n",
    "\n",
    "resultVector_rdd = fakeDataVector_rdd.map(lambda x: 0 if x<=threshold else x )\n",
    "resultVector_rdd.take(1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "requiredTime_pyspark_map = end_time-start_time_map\n",
    "\n",
    "print(\"\\n\\nVectorized \\t{:.6f} seconds \\n\\t\\tImprovement of {:.2f} X over sequential\".\n",
    "      format(requiredTime_vectorized, \n",
    "             requiredTime_sequential/requiredTime_vectorized))\n",
    "\n",
    "print(\"\\n\\nPySpark \\t{:.6f} seconds \\n\\t\\tImprovement of {:,.2f} X over vectorized \\n\\t\\tImprovement of {:,.2f} X over sequential\".\n",
    "      format(requiredTime_pyspark_map, \n",
    "             requiredTime_vectorized/requiredTime_pyspark_map,\n",
    "             requiredTime_sequential/requiredTime_pyspark_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To sum up\n",
    "\n",
    "#### If we take into account data creation & processing we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential \t5.870652 seconds\n",
      "\n",
      "\n",
      "Vectorized \t0.651219 seconds \n",
      "\t\tImprovement of 9.01 X over sequential\n",
      "\n",
      "\n",
      "PySpark \t0.064750 seconds \n",
      "\t\tImprovement of 10.06 X over vectorized \n",
      "\t\tImprovement of 90.67 X over sequential\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequential \\t{:.6f} seconds\".format(requiredTime_sequential+requiredTime_dataCreation))\n",
    "\n",
    "print(\"\\n\\nVectorized \\t{:.6f} seconds \\n\\t\\tImprovement of {:.2f} X over sequential\".\n",
    "      format(requiredTime_vectorized+requiredTime_dataCreation, \n",
    "             (requiredTime_sequential+requiredTime_dataCreation)/(requiredTime_vectorized+requiredTime_dataCreation)))\n",
    "\n",
    "print(\"\\n\\nPySpark \\t{:.6f} seconds \\n\\t\\tImprovement of {:,.2f} X over vectorized \\n\\t\\tImprovement of {:,.2f} X over sequential\".\n",
    "      format(requiredTime_pyspark_overall,\n",
    "             (requiredTime_vectorized+requiredTime_dataCreation)/requiredTime_pyspark_overall,\n",
    "             (requiredTime_sequential+requiredTime_dataCreation)/requiredTime_pyspark_overall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fun fact\n",
    "\n",
    "#### They all run on the same identical hardware... they just get much better at using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
