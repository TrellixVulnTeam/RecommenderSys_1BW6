{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Systems 2016/17\n",
    "\n",
    "### Practice 1 - MapReduce with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we do somenthing to ensure Spark works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999950000\n",
      "714264285\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# Check that Spark is working\n",
    "largeRange = sc.parallelize(range(100000))\n",
    "reduceTest = largeRange.reduce(lambda a, b: a + b)\n",
    "filterReduceTest = largeRange.filter(lambda x: x % 7 == 0).sum()\n",
    "\n",
    "print (reduceTest)\n",
    "print (filterReduceTest)\n",
    "\n",
    "# If the Spark jobs don't work properly these will raise an AssertionError\n",
    "assert reduceTest == 4999950000\n",
    "assert filterReduceTest == 714264285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we create a spark Resilient Distributed Dataset (RDD) containing each line from the file.\n",
    "#### SC is our current SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('2001 A SPACE ODYSSEY.mht')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark provides operations on RDDs to apply transforms produce new RDDs or to return some results.\n",
    "#### Let's experiment a bit... \n",
    "\n",
    "#### First, let's try the map operation. We map each row to its length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 0, 19, 0, 11, 1, 41, 0, 25, 28]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lines_length = lines.map( lambda x: len(x))\n",
    "\n",
    "lines_length.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we apply a reduce operation on the previous result. We reduce the lines length using a sum, obtaining the number of characters in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total file length is: 111887 characters\n"
     ]
    }
   ],
   "source": [
    "total_characters = lines_length.reduce(lambda result,x: (result+x))\n",
    "\n",
    "print (\"Total file length is: {} characters\".format(total_characters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To filter out empty lines we can use a filter transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original lines 5696, non empty lines 4434\n"
     ]
    }
   ],
   "source": [
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "\n",
    "print (\"Original lines {}, non empty lines {}\".format(lines.count(), lines_nonempty.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's remove punctuation marks and transform our data structure made by rows into another one made by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001:',\n",
       " 'a',\n",
       " 'space',\n",
       " 'odyssey',\n",
       " 'screenplay',\n",
       " 'by',\n",
       " 'stanley',\n",
       " 'kubrick',\n",
       " 'and',\n",
       " 'arthur']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_nonempty = lines_nonempty.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ')\\\n",
    "                                    .replace('!',' ').replace('?',' ').lower())\n",
    "\n",
    "words = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we replace each original value in the input RDD with a 2-tuple containing the word in the first position and the integer value 1 in the second position.\n",
    "\n",
    "#### At this point the RDD contains tuples of the form key,value. We create a new RDD containing a tuple for each unique value of key in the input, where the value in the second position of the tuple is created by applying the supplied lambda function to the values with the matching key in the input RDD\n",
    "\n",
    "#### Here the key will be the word and lambda function will sum up the word counts for each word. The output RDD will consist of a single tuple for each unique word in the data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('technique', 1),\n",
       " ('was', 81),\n",
       " ('stirs', 1),\n",
       " ('hungry', 2),\n",
       " ('combats', 1),\n",
       " ('head', 6),\n",
       " ('dark', 3),\n",
       " ('tools', 1),\n",
       " ('b12', 6),\n",
       " ('marshall', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we map a lambda function to the data which will swap over the first and second values in each tuple, now the word count appears in the first position and the word in the second position.\n",
    "\n",
    "#### In the end sort the input RDD by the key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(862, 'the'),\n",
       " (396, 'of'),\n",
       " (381, 'and'),\n",
       " (366, 'to'),\n",
       " (270, 'a'),\n",
       " (197, 'in'),\n",
       " (196, 'it'),\n",
       " (176, 'you'),\n",
       " (162, 'i'),\n",
       " (162, 'is')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = wordcounts.map(lambda x:(x[1],x[0])).sortByKey(ascending=False)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding frequent word bigrams\n",
    "\n",
    "#### A bigram is pair of successive tokens in some sequence. We will look at building bigrams from the sequences of words in each sentence, and then try to find the most frequently occuring ones.\n",
    "\n",
    "#### The first problem is that values in each partition of our initial RDD describe lines from the file rather than sentences. Sentences may be split over multiple lines. The glom() RDD method is used to create a single entry for each document containing the list of all lines, we can then join the lines up, then resplit them into sentences using ( . ) as the separator, using flatMap so that every object in our RDD is now a sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001: a space odyssey           screenplay           by         stanley kubrick and arthur c',\n",
       " ' clark           hawk films ltd',\n",
       " '           c/o',\n",
       " ' m g m studios           boreham wood           herts',\n",
       " '   title         part i          africa          3 000 000 years ago  a1 views of african drylands   drought  the remorseless drought had lasted now for ten million years  and would not end for another million']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('2001 A SPACE ODYSSEY.mht')\n",
    "lines = lines.map( lambda x: x.replace(',',' ').replace('\\t',' ').replace('-',' ')\\\n",
    "                  .replace('!','.').replace('?','.').lower())\n",
    "\n",
    "sentences = lines.glom() \\\n",
    "            .map(lambda x: \" \".join(x)) \\\n",
    "            .flatMap(lambda x: x.split(\".\"))\n",
    "        \n",
    "sentences.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have isolated each sentence we can split it into a list of words and extract the word bigrams from it. Our new RDD contains tuples containing the word bigram (itself a tuple containing the first and second word) as the first value and the number 1 as the second value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2001:', 'a'), 1),\n",
       " (('a', 'space'), 1),\n",
       " (('space', 'odyssey'), 1),\n",
       " (('odyssey', 'screenplay'), 1),\n",
       " (('screenplay', 'by'), 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = sentences.map(lambda x:x.split()) \\\n",
    "    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "\n",
    "bigrams.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we can apply the same reduceByKey and sort steps that we used in the wordcount example, to count up the bigrams and sort them in order of descending frequency. In reduceByKey the key is not an individual word but a bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(90, ('of', 'the')),\n",
       " (49, ('in', 'the')),\n",
       " (41, ('to', 'the')),\n",
       " (32, ('on', 'the')),\n",
       " (32, ('at', 'the')),\n",
       " (31, ('pod', 'bay')),\n",
       " (26, ('and', 'the')),\n",
       " (26, ('it', 'is')),\n",
       " (25, ('to', 'be')),\n",
       " (24, ('we', 'see'))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(ascending=False)\n",
    "    \n",
    "freq_bigrams.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpe parallelization with custom functions\n",
    "\n",
    "### Pi estimation sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141796\n",
      "Computed in 20.626342058181763 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "NUM_SAMPLES = 50000000\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for index in range(0, NUM_SAMPLES):\n",
    "    if inside(index):\n",
    "        count +=1\n",
    "\n",
    "    \n",
    "print (\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "print (\"Computed in {} seconds\".format(time.time() - starting_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pi estimation parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141702\n",
      "Computed in 6.212697744369507 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "NUM_SAMPLES = 50000000\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "    \n",
    "print (\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "print (\"Computed in {} seconds\".format(time.time() - starting_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
